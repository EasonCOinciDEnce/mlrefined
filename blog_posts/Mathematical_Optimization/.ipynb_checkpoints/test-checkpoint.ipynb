{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Optimization Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mathematical optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools of mathematical optimization are designed to find the smallest value(s) of a function, otherwise referred to as a *global minimum* (one smallest point) or *global minima* (many smallest points).  Note that in machine learning / deep learning applications such functions always have *finite global minima*, i.e., their global minima are not negative infinity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next several posts we introduce widely used mathematical optimization algorithms for finding global minima of a function $g\\left(\\mathbf{w}\\right)$. The formal manner of describing the minimization of a function $g$ is written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{w}}{\\mbox{minimize}}\\,\\,\\,\\,g\\left(\\mathbf{w}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "which is simply shorthand for saying ''minimize $g$ over all input values $\\mathbf{w}$''.\n",
    "\n",
    "The algorithms we discuss operate by sequentially decreasing the value of $g$, halting only when a stationary point is found i.e., a point satisfying the first order condition (discussed in the previous post). Thus one can also consider these techniques as numerical methods for solving the first order system of equations $\\nabla g\\left(\\mathbf{w}\\right)=\\mathbf{0}_{N\\times1}$ which - as we discussed previously - one can rarely solve by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  The big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  The big picture\n",
    "\n",
    "All numerical schemes for minimizing a general function $g$ work as follows:\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    " <p>\n",
    " \n",
    "1)  Start the minimization process at some *initial\n",
    "point* $\\mathbf{w}^{0}$.\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "2)  Take iterative steps denoted by $\\mathbf{w}^{1},\\,\\mathbf{w}^{2},\\,\\ldots$,\n",
    "going ``downhill'' towards a stationary point of $g$.\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "3)  Repeat step 2) until the sequence of points converges to a stationary point of\n",
    "$g$.\n",
    "\n",
    "\n",
    " </p>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea is illustrated in figure below for the minimization of a non-convex function. Notice that since this function has three stationary points, the one we reach by traveling downhill depends entirely on where we begin the optimization process. Ideally we would like to find the global minimum, or the lowest of the function's minima, which for a general non-convex function requires that we run the procedure several times with different initializations (or starting points). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src= '../../mlrefined_images/math_optimization_images/Fig_2_5_1.png' width=\"75%\" height=\"100%\"/>\n",
    "</p>\n",
    "<p>\n",
    "<img src= '../../mlrefined_images/math_optimization_images/Fig_2_5_2.png' width=\"75%\" height=\"100%\"/>\n",
    "</p>\n",
    "\n",
    "**Figure 1:** The stationary point of a generic function found via an iterative method of mathematical optimization is dependent on the choice of initial point $w^{0}$. In the top panel our initialization leads us to find the global minimum, while in the bottom panel the two different initializations lead to a saddle point on the left, and a non-global minimum on the right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical methods discussed here halt at a stationary point $\\mathbf{w}$, that is a point where $\\nabla g\\left(\\mathbf{w}\\right)=\\mathbf{0}_{N\\times1}$, which as we have previously seen may or may not constitute a minimum of $g$ if $g$ is non-convex. However this issue does not at all preclude the use of non-convex cost functions in machine learning (or other scientific disciplines), it is simply worth being aware\n",
    "of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Stopping conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of several stopping conditions may be selected to halt numerical algorithms that seek stationary points of a given function\n",
    "$g$. The two most commonly used stopping criteria include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color:rgba(0, 0, 0, 0.0470588); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    " <p>\n",
    " \n",
    "1)  When a pre-specified number of iterations are complete.\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "2)  When the gradient is small enough, i.e., $\\left\\Vert \\nabla g\\left(\\mathbf{w}^{k}\\right)\\right\\Vert _{2}<\\epsilon$\n",
    "for some small $\\epsilon>0$.\n",
    "\n",
    " </p>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most naive stopping condition for a numerical algorithm is to halt the procedure after a pre-defined number of iterations. Note that this extremely simple condition does not provide any convergence guarantee, and hence is typically used in practice in conjunction with other stopping criteria as a necessary cap on the number of iterations when the convergence is achieved slowly. The second condition directly translates our desire to finding a stationary point at which the gradient is by definition zero. One could also stop the procedure when continuing it does not considerably decrease the objective function (or the stationary point itself) from one iteration to the next. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
