{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Optimization Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we describe our first algorithm - random search.  Random search is simple algorithm that works to find the global minimum of a function by - at each step - randomly sampling many directions, evaluating the function at each such sample, and choosing the one that decreases the function value the most.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import matplotlib.pyplot as plt\n",
    "from mlrefined_libraries import basics_library as baslib\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "import autograd.numpy as np\n",
    "%matplotlib notebook\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can introduce here\n",
    "\n",
    "- the notion of a step length, both fixed and diminishing (not sure about adjustable here, besides thats a more advanced topic than I imagine covering at the start of the text)\n",
    "\n",
    "\n",
    "- the notion of moving towards a minimum in the input plane, 1-d and 2-d input examples\n",
    "\n",
    "\n",
    "- the notion of visualizing higher dimensional paths via cost function decrease\n",
    "\n",
    "\n",
    "- the notion of random movements being helpful in overcoming saddle point\n",
    "\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "While it is a good first algorithm to discuss due to its ease of implementation and minimal use of the cost function - we need only be able to evaluate the function itself - random search is typically ineffective for modern machine learning applications.  \n",
    "\n",
    "This is first off because the cost functions we deal with in machine learning have known algebraic forms, thus allowing us to leverage their derivatives to quickly and effeciently determine directions that decrease function value (i.e., we need not search around randomly for directions that do this).  We will see this with the other algorithms we discuss - gradient descent and Newton's method - in significant detail.  Moreover many modern machine learning cost functions have input dimension $N$  on the order of thousands - to hundreds of millions.  In such contexts randomly seeking out direction that substantially decreases a function's value become wildly ineffecient, requiring exponentially more sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
