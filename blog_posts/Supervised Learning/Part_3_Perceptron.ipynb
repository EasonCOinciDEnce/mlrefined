{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: The perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-1e663c61fd35>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-1e663c61fd35>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Introduction to post\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Introduction to post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import matplotlib.pyplot as plt\n",
    "from mlrefined_libraries import superlearn_library as superlearn\n",
    "import autograd.numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  The perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Section we introduce the most foundational tool for two class classification, the \\emph{perceptron}, as well as a popular variation called the \\emph{margin perceptron}. Both tools are commonly used and perform similarly in practice, as we discuss further in Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the previous Chapter that in a linear regression setting, given a training set of $P$ continuous-valued input/output data points $\\left\\{ \\left(\\mathbf{x}_{p},y_{p}\\right)\\right\\} _{p=1}^{P}$ we\n",
    "aim to learn a hyperplane $b+\\mathbf{x}^{T}\\mathbf{w}$ with parameters $b$ and $\\mathbf{w}$ such that \n",
    "\n",
    "\\begin{equation}\n",
    "b+\\mathbf{x}_{p}^{T}\\mathbf{w}\\approx y_{p}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "holds for $p=1,...,P$. In the case of linear classification a disparate yet simple motivation leads to the pursuit of a different sort of ideal hyperplane. As opposed to linear regression, where our aim is\n",
    "to \\textit{represent} a dataset, with classification our goal is to \\textit{separate} two distinct classes of the input/output data with a learned hyperplane. In other words, we want to learn a hyperplane\n",
    "$b+\\mathbf{x}^{T}\\mathbf{w}=0$ that separates the two classes of points as much as possible, with one class lying 'above' the hyperplane in the half-space given by $b+\\mathbf{x}^{T}\\mathbf{w}>0$ and the\n",
    "other 'below' it in the half-space $b+\\mathbf{x}^{T}\\mathbf{w}<0$, as illustrated in Figure \\ref{fig:two-class above/below}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-495666d81063>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-495666d81063>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    FIGURE 4.1 GOES HERE\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "FIGURE 4.1 GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, with two class classification we still have a training set of $P$ input/output data points $\\left\\{ \\left(\\mathbf{x}_{p},y_{p}\\right)\\right\\} _{p=1}^{P}$ where each input $\\mathbf{x}_{p}$ is $N$-dimensional (with each entry representing an input feature, just as with regression). However\n",
    "the output data no longer takes on continuous but two discrete values or \\textit{labels} indicating class membership, i.e., points belonging to each class are assigned a distinct label. While one can choose any two values for this purpose we will see that the values $\\pm1$ are particularly useful and therefore will assume that $y_{p}\\in\\left\\{ -1,\\,+1\\right\\} $ for $p=1,...,P$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to learn the parameters b and \\mathbf{w} of a hyperplane so that the first class (where y_{p}=+1) lies largely above the hyperplane in the half-space defined by b+\\mathbf{x}^{T}\\mathbf{w}>0, and the second class (where y_{p}=-1) lies mostly below it in the half-space defined by $b+\\mathbf{x}^{T}\\mathbf{w}<0$.\\textit{\\emph{If a given hyperplane places the point $\\mathbf{x}_{p}$ on its correct\n",
    "side (or we say that it correctly classifies the point) then we have precisely that\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{cc}\n",
    "b+\\mathbf{x}_{p}^{T}\\mathbf{w}>0 & \\,\\,\\mbox{if }\\,\\,\\mbox{\\ensuremath{y_{p}=+1}}\\\\\n",
    "b+\\mathbf{x}_{p}^{T}\\mathbf{w}<0 & \\,\\,\\mbox{if}\\,\\,\\mbox{\\ensuremath{y_{p}=-1}.}\n",
    "\\end{array}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have chosen the labels $\\pm1$ we can express (\\ref{eq:original-separation-criteria})\n",
    "compactly by multiplying the two expressions by minus their respective\n",
    "label value $-y_{p}$, giving one equivalent expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "-y_{p}^{\\,}\\left(b+\\mathbf{x}_{p}^{T}\\mathbf{w}\\right)<0.\\label{eq:correctly-classified-point}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the maximum of this quantity and zero we can then write\n",
    "this condition, which states that a hyperplane correctly classifies\n",
    "the point $\\mathbf{x}_{p}$, equivalently as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{aligned}\\mbox{max}\\left(0,\\,-y_{p}\\left(b+\\mathbf{x}_{p}^{T}\\mathbf{w}\\right)\\right)=0.\\end{aligned}\n",
    "\\label{eq:perceptron-criterion}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the expression $\\mbox{max}\\left(0,\\,-y_{p}^{\\,}\\left(b+\\mathbf{x}_{p}^{T}\\mathbf{w}\\right)\\right)$\n",
    "returns zero if \\textit{\\emph{$\\mathbf{x}_{p}$ is classified correctly,\n",
    "but it returns a }}\\textit{positive}\\textit{\\emph{ value }}if the\n",
    "point is classified incorrectly. This is useful not only because it\n",
    "characterizes the sort of hyperplane we wish to have, but more importantly\n",
    "by simply summing this expression over all the points we have the\n",
    "non-negative cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g_{1}\\left(b,\\mathbf{w}\\right)=\\underset{p=1}{\\overset{P}{\\sum}}\\mbox{max}\\left(0,\\,-y_{p}\\left(b+\\mathbf{x}_{p}^{T}\\mathbf{w}\\right)\\right),\\label{eq:perceptron_cost}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "referred to as the perceptron or max cost function \n",
    "\n",
    "The perceptron is also referred to as the hinge (as it is shaped like a hinge, see Figure [fig: softmax-vs-max] for an illustration) or rectified linear unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The choice of which class we assume lies 'above' and 'below' the hyperplane is arbitrary, i.e., if we instead suppose that those points with label y_{p}=-1 lie above and those with label y_{p}=+1 lie below similar calculations can be made which lead to the perceptron cost function in equation ([eq:perceptron_cost]). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "85px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
